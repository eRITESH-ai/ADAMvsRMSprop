{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28cc59d",
   "metadata": {},
   "source": [
    "TEAM 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fd8e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\python311\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python311\\lib\\site-packages (from gymnasium) (2.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\python311\\lib\\site-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\python311\\lib\\site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\python311\\lib\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ca41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56adbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa10b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d3b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer_name=\"adam\", seed=0, episodes=500):\n",
    "    set_seed(seed)\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "    gamma = 0.99\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            logits, value = model(state_tensor)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state_tensor = torch.FloatTensor(next_state)\n",
    "            _, next_value = model(next_state_tensor)\n",
    "\n",
    "            target = reward + gamma * next_value * (1 - done)\n",
    "            advantage = target - value\n",
    "\n",
    "            actor_loss = -dist.log_prob(action) * advantage.detach()\n",
    "            critic_loss = advantage.pow(2)\n",
    "\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return rewards_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5f5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(optimizer_name, seeds=5):\n",
    "    all_rewards = []\n",
    "    for s in range(seeds):\n",
    "        rewards = train(optimizer_name=optimizer_name, seed=s)\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    return np.array(all_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd67725",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_results = run_experiment(\"adam\", seeds=5)\n",
    "rms_results = run_experiment(\"rmsprop\", seeds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54359002",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adam_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     plt.legend()\n\u001b[32m     26\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m plot_results(\u001b[43madam_results\u001b[49m, rms_results)\n",
      "\u001b[31mNameError\u001b[39m: name 'adam_results' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_results(adam, rms):\n",
    "    adam_mean = adam.mean(axis=0)\n",
    "    adam_std = adam.std(axis=0)\n",
    "\n",
    "    rms_mean = rms.mean(axis=0)\n",
    "    rms_std = rms.std(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    plt.plot(adam_mean, label=\"Adam\")\n",
    "    plt.fill_between(range(len(adam_mean)),\n",
    "                     adam_mean - adam_std,\n",
    "                     adam_mean + adam_std,\n",
    "                     alpha=0.2)\n",
    "\n",
    "    plt.plot(rms_mean, label=\"RMSprop\")\n",
    "    plt.fill_between(range(len(rms_mean)),\n",
    "                     rms_mean - rms_std,\n",
    "                     rms_mean + rms_std,\n",
    "                     alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Adam vs RMSprop on CartPole\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(adam_results, rms_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Run experiments to generate results\n",
    "adam_results = run_experiment(\"adam\", seeds=5)\n",
    "rms_results = run_experiment(\"rmsprop\", seeds=5)\n",
    "\n",
    "# ---------------------------\n",
    "# AUC (Area Under the Curve)\n",
    "# ---------------------------\n",
    "def calculate_auc(results):\n",
    "    return results.sum(axis=1)\n",
    "\n",
    "adam_auc = calculate_auc(adam_results)\n",
    "rms_auc = calculate_auc(rms_results)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Convergence Speed\n",
    "# ---------------------------\n",
    "def convergence_speed(results, threshold=475):\n",
    "    speeds = []\n",
    "    for run in results:\n",
    "        reached = False\n",
    "        for i, reward in enumerate(run):\n",
    "            if reward >= threshold:\n",
    "                speeds.append(i)\n",
    "                reached = True\n",
    "                break\n",
    "        if not reached:\n",
    "            speeds.append(len(run))\n",
    "    return np.array(speeds)\n",
    "\n",
    "adam_speed = convergence_speed(adam_results)\n",
    "rms_speed = convergence_speed(rms_results)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Final Performance\n",
    "# ---------------------------\n",
    "def final_performance(results, window=50):\n",
    "    return results[:, -window:].mean(axis=1)\n",
    "\n",
    "adam_final = final_performance(adam_results)\n",
    "rms_final = final_performance(rms_results)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Stability (Variance)\n",
    "# ---------------------------\n",
    "adam_variance = adam_results.var(axis=1)\n",
    "rms_variance = rms_results.var(axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Early Learning\n",
    "# ---------------------------\n",
    "def early_performance(results, window=100):\n",
    "    return results[:, :window].mean(axis=1)\n",
    "\n",
    "adam_early = early_performance(adam_results)\n",
    "rms_early = early_performance(rms_results)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Statistical Test\n",
    "# ---------------------------\n",
    "t_stat, p_value = ttest_ind(adam_final, rms_final)\n",
    "\n",
    "# ---------------------------\n",
    "# PRINT RESULTS\n",
    "# ---------------------------\n",
    "print(\"----- METRIC VALUES -----\")\n",
    "print(\"AUC: Adam =\", adam_auc.mean(), \" | RMSprop =\", rms_auc.mean())\n",
    "print(\"Final Reward: Adam =\", adam_final.mean(), \" | RMSprop =\", rms_final.mean())\n",
    "print(\"Convergence Episodes: Adam =\", adam_speed.mean(), \" | RMSprop =\", rms_speed.mean())\n",
    "print(\"Variance: Adam =\", adam_variance.mean(), \" | RMSprop =\", rms_variance.mean())\n",
    "print(\"Early Learning: Adam =\", adam_early.mean(), \" | RMSprop =\", rms_early.mean())\n",
    "print(\"P-value (final reward):\", p_value)\n",
    "\n",
    "# ---------------------------\n",
    "# Determine Winner Per Metric\n",
    "# ---------------------------\n",
    "score_adam = 0\n",
    "score_rms = 0\n",
    "\n",
    "print(\"\\n----- WINNER PER METRIC -----\")\n",
    "\n",
    "# AUC (higher better)\n",
    "if adam_auc.mean() > rms_auc.mean():\n",
    "    print(\"AUC: Adam better\")\n",
    "    score_adam += 1\n",
    "else:\n",
    "    print(\"AUC: RMSprop better\")\n",
    "    score_rms += 1\n",
    "\n",
    "# Final Reward (higher better)\n",
    "if adam_final.mean() > rms_final.mean():\n",
    "    print(\"Final Reward: Adam better\")\n",
    "    score_adam += 1\n",
    "else:\n",
    "    print(\"Final Reward: RMSprop better\")\n",
    "    score_rms += 1\n",
    "\n",
    "# Convergence Speed (lower better)\n",
    "if adam_speed.mean() < rms_speed.mean():\n",
    "    print(\"Convergence Speed: Adam better\")\n",
    "    score_adam += 1\n",
    "else:\n",
    "    print(\"Convergence Speed: RMSprop better\")\n",
    "    score_rms += 1\n",
    "\n",
    "# Variance (lower better)\n",
    "if adam_variance.mean() < rms_variance.mean():\n",
    "    print(\"Stability (Variance): Adam better\")\n",
    "    score_adam += 1\n",
    "else:\n",
    "    print(\"Stability (Variance): RMSprop better\")\n",
    "    score_rms += 1\n",
    "\n",
    "# Early Learning (higher better)\n",
    "if adam_early.mean() > rms_early.mean():\n",
    "    print(\"Early Learning: Adam better\")\n",
    "    score_adam += 1\n",
    "else:\n",
    "    print(\"Early Learning: RMSprop better\")\n",
    "    score_rms += 1\n",
    "\n",
    "# ---------------------------\n",
    "# Overall Winner\n",
    "# ---------------------------\n",
    "print(\"\\n----- OVERALL RESULT -----\")\n",
    "\n",
    "if score_adam > score_rms:\n",
    "    print(\"Overall Winner on CartPole: ADAM\")\n",
    "elif score_rms > score_adam:\n",
    "    print(\"Overall Winner on CartPole: RMSPROP\")\n",
    "else:\n",
    "    print(\"Overall Result: Tie / Comparable Performance\")\n",
    "\n",
    "print(\"\\nScore -> Adam:\", score_adam, \"| RMSprop:\", score_rms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
